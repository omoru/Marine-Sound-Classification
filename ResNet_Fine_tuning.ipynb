{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_VFh3irFVm6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from os.path import join\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "#from tensorflow.python.keras.applications import ResNet50\n",
        "\n",
        "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils, to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7cPla-bFnpU"
      },
      "outputs": [],
      "source": [
        "# Convoluted Base MODEL\n",
        "\n",
        "conv_base = ResNet50(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3))\n",
        "print(conv_base.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm1xssB_GcJQ"
      },
      "outputs": [],
      "source": [
        "# MODEL\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j1OSkUOGiu7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make the conv_base NOT trainable:\n",
        "\n",
        "for layer in conv_base.layers[:]:\n",
        "   layer.trainable = False\n",
        "\n",
        "print('conv_base is now NOT trainable')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMonWvH0Gshs"
      },
      "outputs": [],
      "source": [
        "# Compile frozen conv_base + my top layer\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer= optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "print(\"model compiled\")\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5MDTo9qeQ4Q"
      },
      "source": [
        "Leer datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_gtzNSvHa7E"
      },
      "outputs": [],
      "source": [
        "# dataset1 = pd.read_csv(\"/content/drive/MyDrive/Dataton/finaldf.csv\")\n",
        "dataset1 = pd.read_csv(\"/content/drive/MyDrive/Dataton/labels_dataset1_v2.csv\")\n",
        "dataset1.drop_duplicates(inplace=True)\n",
        "dataset1.reset_index(inplace=True,drop=True)\n",
        "dataset1['image_name'] = [str(i) + '.jpg' for i in range(len(dataset1))]\n",
        "print(dataset1.label.value_counts())\n",
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikOAfccdD6wP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
        "  \"\"\"\n",
        "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
        "  Some examples of different formats of class_series and their outputs are:\n",
        "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
        "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
        "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
        "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
        "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
        "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
        "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
        "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
        "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
        "  of appareance of the label when the dataset was processed. \n",
        "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
        "  Author: Angel Igareta (angel@igareta.com)\n",
        "  \"\"\"\n",
        "  if multi_class:\n",
        "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
        "    if one_hot_encoded:\n",
        "      class_series = np.argmax(class_series, axis=1)\n",
        "  \n",
        "    # Compute class weights with sklearn method\n",
        "    class_labels = np.unique(class_series)\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
        "    return dict(zip(class_labels, class_weights))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93e1GmLjD72h"
      },
      "outputs": [],
      "source": [
        "class_weights = generate_class_weights(dataset1['label'])\n",
        "class_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNNoF4hpeSqY"
      },
      "source": [
        "Crear conjunto test,train,val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEfpccaaHC3F"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# a = random.randint(0,500)\n",
        "# print(a)\n",
        "# random_state = a\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 261\n",
        "X_train, X_test = train_test_split(dataset1,test_size=0.10, random_state=random_state)\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=random_state)\n",
        "\n",
        "directory = '/content/drive/MyDrive/Dataton/datos/all'\n",
        "batch_size = 16\n",
        "target_size = (224,224)\n",
        "datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "train_generator=datagen.flow_from_dataframe(\n",
        "    dataframe=X_train,\n",
        "    directory=directory,\n",
        "    x_col=\"image_name\",\n",
        "    y_col=\"label\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=target_size)\n",
        "\n",
        "validation_generator=datagen.flow_from_dataframe(\n",
        "    dataframe=X_val,\n",
        "    directory=directory,\n",
        "    x_col=\"image_name\",\n",
        "    y_col=\"label\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=target_size)\n",
        "\n",
        "test_generator=datagen.flow_from_dataframe(\n",
        "    dataframe=X_test,\n",
        "    directory=directory,\n",
        "    x_col=\"image_name\",\n",
        "    y_col=\"label\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=target_size)\n",
        "\n",
        "print(validation_generator.image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6VOw9H3IXNn"
      },
      "outputs": [],
      "source": [
        "# Short training ONLY my top layers \n",
        "#... so the conv_base weights will not be destroyed by the random intialization of the new weights\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    class_weight = dictio,\n",
        "                    epochs=1,\n",
        "                    steps_per_epoch = train_generator.samples // batch_size,\n",
        "                    validation_data = validation_generator,\n",
        "                    validation_steps = validation_generator.samples // batch_size)\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TBxMLeFaeFW"
      },
      "outputs": [],
      "source": [
        "#probar tiempo-------------------------------------------------------\n",
        "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)):\n",
        "  history = model.fit(train_generator,\n",
        "                                epochs=1,\n",
        "                                steps_per_epoch = train_generator.samples // batch_size,\n",
        "                                validation_data = validation_generator,\n",
        "                                validation_steps = validation_generator.samples // batch_size)\n",
        "                                # workers = 4, use_multiprocessing = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1qNdErIIhRn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Make last block of the conv_base trainable:\n",
        "for layer in conv_base.layers[:165]:\n",
        "   layer.trainable = False\n",
        "for layer in conv_base.layers[165:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "print('Last block of the conv_base is now trainable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xn6jNt2Il81"
      },
      "outputs": [],
      "source": [
        "# Compile frozen conv_base + UNfrozen top block + my top layer ... SLOW LR\n",
        "# Compiling using adam and categorical crossentropy\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer= optimizers.Adam(learning_rate=1e-5),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EawYRRZEITU3"
      },
      "outputs": [],
      "source": [
        "# Long training with fine tuning\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "history = model.fit(train_generator, callbacks=[model_checkpoint_callback],\n",
        "                              epochs=50,\n",
        "                              steps_per_epoch = train_generator.samples // batch_size,\n",
        "                              validation_data = validation_generator,\n",
        "                              validation_steps = validation_generator.samples // batch_size,workers = 4, use_multiprocessing = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTWEPsZyH2kE"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-20clLYIxJ0"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator, steps= test_generator.samples // batch_size, verbose=1)\n",
        "print('test acc:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm7oskETIztt"
      },
      "outputs": [],
      "source": [
        "# SAVE or LOAD model (Keras - all batteries included: architecture, weights, optimizer, last status in training, etc.)\n",
        "# YOU supply this model.h5 file from previous training session(s) - expected as a data source by Kaggle\n",
        "\n",
        "# SAVE model\n",
        "model.save('ResNet50FineTune.h5')\n",
        "print(\"ResNet50FineTune.h5 was saved\")\n",
        "\n",
        "# LOAD model\n",
        "#del model\n",
        "#model = load_model('')\n",
        "#print(\"ResNet50FineTune.h5 was loaded\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
