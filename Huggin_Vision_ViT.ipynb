{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rym0aobU0hvQ"},"outputs":[],"source":["!pip install hugsvision\n","from hugsvision.dataio.VisionDataset import VisionDataset\n","import pandas as pd\n","import numpy as np\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from hugsvision.nnet.VisionClassifierTrainer import VisionClassifierTrainer\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","from hugsvision.inference.VisionClassifierInference import VisionClassifierInference\n","import soundfile as sf\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-aS5OYy5li8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Crea y entrena modelo\n","Solo para entrenamiento. Para realizar la inferencia sobre los audios de validacion pasar al siguiente punto"],"metadata":{"id":"zqg-BAQCNPkE"}},{"cell_type":"code","source":["huggingface_model = 'google/vit-base-patch16-224-in21k'\n","\n","#Particiona datos\n","# train, test, id2label, label2id = VisionDataset.fromImageFolder(\n","#   \"/content/drive/MyDrive/Dataton/datos/por_carpetas/\",\n","#   test_ratio   = 0.2,\n","#   balanced     = False,\n","#   augmentation = False,\n","# )\n","#Particiona datos\n","train, test, id2label, label2id = VisionDataset.fromImageFolders(\n","  train = \"/content/drive/MyDrive/augmented2\",\n","\ttest = \"/content/drive/MyDrive/original2\",\n",")\n","\n"],"metadata":{"id":"dsG6PZi-0mBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = VisionClassifierTrainer(\n","\tmodel_name   = \"ViT_MarineFinal\",\n","\ttrain        = train,\n","\ttest         = test,\n","\toutput_dir   = \"/content/drive/MyDrive/models/\",\n","\tmax_epochs   = 10,\n","\tbatch_size   = 16, \n","\tlr\t     = 1e-5,\n","\tfp16\t     = True,\n","\tmodel = ViTForImageClassification.from_pretrained(\n","\t    huggingface_model,\n","\t    num_labels = len(label2id),\n","\t    label2id   = label2id,\n","\t    id2label   = id2label,\n","\t),\n","\tfeature_extractor = ViTFeatureExtractor.from_pretrained(\n","\t\thuggingface_model,\n","\t),\n",")"],"metadata":{"id":"gz8hhMMCQsSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hyp, ref = trainer.evaluate_f1_score()"],"metadata":{"id":"p1SBj6E_AtOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inferencia"],"metadata":{"id":"lN3_-N8lOyLz"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/Modelo/VIT_MARINEFINAL/10_2022-10-25-00-42-10/model\"\n","fe_path = \"/content/drive/MyDrive/Modelo/VIT_MARINEFINAL/10_2022-10-25-00-42-10/feature_extractor\"\n","\n","classifier = VisionClassifierInference(\n","    feature_extractor = ViTFeatureExtractor.from_pretrained(fe_path),\n","    model = ViTForImageClassification.from_pretrained(model_path),\n",")\n"],"metadata":{"id":"GOIY-djDEw2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definicion función de ventanas para calcular los tiempos de inicio y fin:"],"metadata":{"id":"aCzkyeJImKso"}},{"cell_type":"code","source":["\n","PERIOD = 10 #Tamaño ventana en segundos (aumentar lo hace mas rapido pero menos exacto)\n","SALTO = 5 #Desplazamiento de la ventana en segundos\n","\n","def genera_subclips(clip,sr):\n","  audios = []\n","  len_y = len(clip)\n","  start = 0\n","  end = PERIOD * sr\n","  while True:\n","        y_batch = clip[start:end].astype(np.float32)\n","        if len(y_batch) != PERIOD * sr:\n","            y_pad = np.zeros(PERIOD * sr, dtype=np.float32)\n","            y_pad[:len(y_batch)] = y_batch\n","            audios.append(y_pad)\n","            break\n","        start += SALTO * sr\n","        end += SALTO * sr\n","        audios.append(y_batch)\n","  return audios\n","\n","\n","def calcula_anotaciones(estimated_event_list,sr,filename):\n","  anotaciones = []\n","  len_events = len(estimated_event_list)\n","  i = 0\n","  while i < len_events:\n","    j = i + 1\n","    while j < len_events and estimated_event_list[i] == estimated_event_list[j]:\n","      j+=1\n","    if estimated_event_list[i]!='silence':\n","      if j == len_events:\n","        anotaciones.append({'path': filename,\n","                  'start':float(SALTO * i),\n","                  'end':float(SALTO * (j-1) + PERIOD),\n","                  'label':estimated_event_list[j-1]})\n","      else:\n","          anotaciones.append({'path': filename,\n","                'start':float(SALTO * i),\n","                'end':float(SALTO * j),\n","                'label':estimated_event_list[j-1]})\n","    i = j\n","  \n","  return anotaciones\n","\n","def prediction_for_clip(filename,clip: np.ndarray,sr,threshold=0.5):\n","\n","     #Generamos los subaudios del audio\n","    print('Duración clip: ' + str(len(clip)/sr) + ' segundos')\n","    y = clip.astype(np.float32)\n","    audios = genera_subclips(y,sr)\n","    print('Ventanas de ' + str(PERIOD) +' segundos')\n","    print('N ventanas: ' + str(len(audios)))\n","\n","    estimated_event_list = []\n","    for audio in audios:\n","      #Generar espectrogramas\n","      fig = plt.figure(figsize=[1,1])\n","      # This is to get rid of the axes and only get the picture \n","      ax = fig.add_subplot(111)\n","      ax.axes.get_xaxis().set_visible(False)\n","      ax.axes.get_yaxis().set_visible(False)\n","      ax.set_frame_on(False)\n","      # This is the melspectrogram from the decibels with a linear relationship\n","      S = librosa.feature.melspectrogram(y=audio, sr=sr)\n","      librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='linear')\n","      # Here we choose the path and the name to save the file, we will change the path when\n","      # using the function for train, val and test to make the function easy to use and output\n","      # the images in different folders to use later with a generator\n","\n","      file_spectrogram  = '/content/iterator.jpg'\n","      # Here we finally save the image file choosing the resolution \n","      plt.savefig(file_spectrogram, dpi=500, bbox_inches='tight',pad_inches=0)\n","      \n","      # Here we close the image because otherwise we get a warning saying that the image stays\n","      # open and consumes memory\n","      plt.close()\n","            \n","      #PREDICT\n","      y_pred = classifier.predict(img_path=file_spectrogram,return_str=False)\n","      outputs = np.array(list(y_pred.values()))\n","      result_label = list(y_pred.keys())[outputs.argmax()]\n","      result_score = max(outputs)/100 \n","      \n","      if (result_score > threshold):\n","        estimated_event_list.append(result_label)\n","      else:\n","        estimated_event_list.append('silence')\n","    return calcula_anotaciones(estimated_event_list,sr,filename)\n","      \n","  "],"metadata":{"id":"lf-v1tj022zX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calcular anotaciones en un conjunto de audios -> **indicar en submision_audios la carpeta en la que se encuentran los audios a anotar**"],"metadata":{"id":"P94wf2WSmRxP"}},{"cell_type":"code","source":["import os\n","submision_audios = '/content/drive/MyDrive/Dataton/submission'# Carpeta que contiene los audios\n","final_df = pd.DataFrame()\n","i=0\n","for audio_name in os.listdir(submision_audios):\n","  print(f'Audio nº {i}')\n","  clip, sr = sf.read(submision_audios + '/' + audio_name)\n","  anotaciones = prediction_for_clip(audio_name,clip,sr,threshold = 0.5)\n","  df_anotaciones = pd.DataFrame(anotaciones)\n","  final_df = pd.concat([final_df, df_anotaciones], ignore_index=True)\n","  print(\"audio \"+ audio_name + \" procesado---->\" + str(len(final_df))+ \"anotaciones totales\")\n","  i+=1"],"metadata":{"id":"yb4HiBX91Y5M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Guardamos el dataset de anotaciones"],"metadata":{"id":"ktLWcEonmeyz"}},{"cell_type":"code","source":["final_df.to_csv('/content/drive/MyDrive/Dataton/pred.csv',index = False)\n","final_df"],"metadata":{"id":"OcowEZUumuOK"},"execution_count":null,"outputs":[]}]}